{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install keras\n",
        "!pip install pandas\n",
        "!pip install matplotlib\n",
        "!pip install scikit-learn\n",
        "!pip install seaborn\n",
        "!pip install tensorboard\n",
        "!pip install numpy"
      ],
      "metadata": {
        "collapsed": true,
        "id": "UAGcoghBSzVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD,Adam, RMSprop\n",
        "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, LearningRateScheduler\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime"
      ],
      "metadata": {
        "id": "BPGWNubOX7yO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the march_training_data.parquet, may_training_data.parquet, and March 2020 climatic dataset.csv, may_climatic_data.csv, and display the data\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Parquet files\n",
        "march_data = pd.read_parquet('march_training_data.parquet')\n",
        "may_data = pd.read_parquet('may_training_data.parquet')\n",
        "\n",
        "# Load the CSV files\n",
        "march_climate = pd.read_csv('March 2020 climatic dataset.csv')\n",
        "may_climate = pd.read_csv('may_climatic_data.csv')\n",
        "\n",
        "# Display the data (you can adjust the number of rows displayed with n=...)\n",
        "print(\"March Training Data:\")\n",
        "display(march_data)\n",
        "\n",
        "print(\"\\nMay Training Data:\")\n",
        "display(may_data)\n",
        "\n",
        "print(\"\\nMarch Climatic Data:\")\n",
        "display(march_climate.head())\n",
        "\n",
        "print(\"\\nMay Climatic Data:\")\n",
        "display(may_climate.head())"
      ],
      "metadata": {
        "id": "_WluqZVXFezM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check for and fill Missing Values in climatic dataset"
      ],
      "metadata": {
        "id": "HVuqxUlYmwMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Check for  Missing values in march and may climatic datasets\n",
        "\n",
        "# Check for missing values in March climatic data\n",
        "print(\"\\nMissing values in March climatic data:\")\n",
        "print(march_climate.isnull().sum())\n",
        "\n",
        "# Check for missing values in May climatic data\n",
        "print(\"\\nMissing values in May climatic data:\")\n",
        "print(may_climate.isnull().sum())"
      ],
      "metadata": {
        "id": "P5s50yfQIGHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop columns with more than 75% missing values\n",
        "\n",
        "# Calculate the threshold for missing values (75% of the rows)\n",
        "threshold = len(march_climate)*0.75\n",
        "\n",
        "# Drop columns with more than 50% missing values in march_climate\n",
        "march_climate.dropped = march_climate.dropna(thresh=threshold, axis=1)\n",
        "\n",
        "# Drop columns with more than 50% missing values in may_climate\n",
        "may_climate.dropped = may_climate.dropna(thresh=threshold, axis=1)\n",
        "\n",
        "print(\"\\nMarch Climatic Data after dropping columns:\")\n",
        "#display(march_climate.head())\n",
        "print(march_climate.dropped.shape)\n",
        "display(march_climate.dropped)\n",
        "\n",
        "print(\"\\nMay Climatic Data after dropping columns:\")\n",
        "#display(may_climate.head())\n",
        "print(may_climate.dropped.shape)\n",
        "display(may_climate.dropped)"
      ],
      "metadata": {
        "id": "Ahm6GTHTIV7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Fill missing values in 'wspd' and 'pres' columns using forward fill and mean imputation in March Climatic Data\n",
        "\n",
        "# Fill missing 'wspd' values using forward fill\n",
        "march_climate.dropped.loc[:, 'wspd'] = march_climate.dropped['wspd'].ffill()\n",
        "# Fill missing 'pres' values using the mean\n",
        "mean_pres = march_climate.dropped['pres'].mean() # Calculate mean_pres first\n",
        "march_climate.dropped.loc[:, 'pres'] = march_climate.dropped['pres'].fillna(mean_pres) # Now use mean_pres to fill NA\n",
        "\n",
        "\n",
        "print(\"\\nMarch Climatic Data after filling missing values:\")\n",
        "display(march_climate.dropped)"
      ],
      "metadata": {
        "id": "FLf18m6oK5g_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract just the date from the tpep_pickup_datetime in march and may training dataset, and display them\n",
        "\n",
        "# Extract the date part from the pickup_datetime column\n",
        "march_data['pickup_date'] = pd.to_datetime(march_data['tpep_pickup_datetime']).dt.date\n",
        "may_data['pickup_date'] = pd.to_datetime(may_data['tpep_pickup_datetime']).dt.date\n",
        "\n",
        "# Display the extracted dates\n",
        "print(\"March Pickup Dates:\")\n",
        "display(march_data['pickup_date'].head())\n",
        "\n",
        "print(\"\\nMay Pickup Dates:\")\n",
        "display(may_data['pickup_date'].head())"
      ],
      "metadata": {
        "id": "HKETG3UoUrm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'pickup_date' column to datetime format\n",
        "march_data['pickup_date'] = pd.to_datetime(march_data['pickup_date'])\n",
        "print(march_data['pickup_date'].dtype)\n",
        "may_data['pickup_date'] = pd.to_datetime(may_data['pickup_date'])\n",
        "print(may_data['pickup_date'].dtype)"
      ],
      "metadata": {
        "id": "KKn7a7DR9M5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the 'date' column to datetime format in May and March Climatic Data\n",
        "\n",
        "# Convert the 'date' column to datetime format in march_climate and may_climate\n",
        "march_climate.dropped['date'] = pd.to_datetime(march_climate.dropped['date'])\n",
        "may_climate.dropped['date'] = pd.to_datetime(may_climate.dropped['date'])\n",
        "\n",
        "print(\"\\nMarch Climatic Data after converting 'date' to datetime:\")\n",
        "display(march_climate.dropped)\n",
        "\n",
        "print(\"\\nMay Climatic Data after converting 'date' to datetime:\")\n",
        "display(may_climate.dropped)"
      ],
      "metadata": {
        "id": "ouB1ArbgXVG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if 'date is in datetime format, and display the the dtye 'date' column\n",
        "\n",
        "# Check the data type of the 'date' column in march_climate.dropped\n",
        "print(march_climate.dropped['date'].dtype)\n",
        "\n",
        "# Display the 'date' column\n",
        "display(march_climate.dropped['date'])\n",
        "\n",
        "# Check the data type of the 'date' column in may_climate.dropped\n",
        "print(may_climate.dropped['date'].dtype)\n",
        "\n",
        "# Display the 'date' column\n",
        "display(may_climate.dropped['date'])"
      ],
      "metadata": {
        "id": "KE6MygXIYJew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge the march and nay dataset with the march and may climatic dataset on 'date'\n",
        "\n",
        "# Merge the datasets\n",
        "march_merged = pd.merge(march_data, march_climate.dropped, left_on='pickup_date', right_on='date', how='left')\n",
        "may_merged = pd.merge(may_data, may_climate.dropped, left_on='pickup_date', right_on='date', how='left')\n",
        "\n",
        "print(\"\\nMarch Merged Data:\")\n",
        "display(march_merged)\n",
        "\n",
        "print(\"\\nMay Merged Data:\")\n",
        "display(may_merged)"
      ],
      "metadata": {
        "id": "26rFm6J3YZDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge march_merged data and may_merged data while keeping all the date\n",
        "\n",
        "# Concatenate the merged datasets\n",
        "merged_data = pd.concat([march_merged, may_merged], ignore_index=True)\n",
        "\n",
        "print(\"\\nMerged Data (March and May):\")\n",
        "display(merged_data)"
      ],
      "metadata": {
        "id": "tLqJAi_aZGzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop 'pickup_date', 'date' columns in Merged Data\n",
        "\n",
        "# Drop 'pickup_date' and 'date' columns from the merged data\n",
        "merged_data = merged_data.drop(['pickup_date', 'date'], axis=1)\n",
        "\n",
        "print(\"\\nMerged Data after dropping columns:\")\n",
        "display(merged_data)"
      ],
      "metadata": {
        "id": "Y1A8wcyPzK6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare Data for Modeling\n",
        "### identify numerical features, and scale them\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0dBbQiR-DvaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'tpep_pickup_datetime' and 'tpep_dropoff_datetime' to datetime objects\n",
        "merged_data['tpep_pickup_datetime'] = pd.to_datetime(merged_data['tpep_pickup_datetime'])\n",
        "merged_data['tpep_dropoff_datetime'] = pd.to_datetime(merged_data['tpep_dropoff_datetime'])\n"
      ],
      "metadata": {
        "id": "IHkWFmhx9xNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Extract day_of_week, hour, month from 'tpep_pickup_datetime' and 'tpep_dropoff_datetime', and disply the updated dataset\n",
        "\n",
        "# Extract day_of_week, hour, and month from 'tpep_pickup_datetime'\n",
        "merged_data['pickup_day_of_week'] = merged_data['tpep_pickup_datetime'].dt.dayofweek\n",
        "merged_data['pickup_hour'] = merged_data['tpep_pickup_datetime'].dt.hour\n",
        "merged_data['pickup_month'] = merged_data['tpep_pickup_datetime'].dt.month\n",
        "\n",
        "# Extract day_of_week, hour, and month from 'tpep_dropoff_datetime'\n",
        "merged_data['dropoff_day_of_week'] = merged_data['tpep_dropoff_datetime'].dt.dayofweek\n",
        "merged_data['dropoff_hour'] = merged_data['tpep_dropoff_datetime'].dt.hour\n",
        "merged_data['dropoff_month'] = merged_data['tpep_dropoff_datetime'].dt.month\n",
        "\n",
        "# Display the updated dataset\n",
        "display(merged_data)"
      ],
      "metadata": {
        "id": "RK_CJl8v8SNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify Numerical Features in merged data for Scaling,\n",
        "\n",
        "# Identify numerical features\n",
        "numerical_features = merged_data.select_dtypes(include=['number']).columns\n",
        "\n",
        "# Remove 'trip_duration' from numerical_features as it's the target variable\n",
        "numerical_features = numerical_features.drop('trip_duration')\n",
        "\n",
        "print(\"Numerical Features:\")\n",
        "numerical_features\n",
        "display(merged_data[numerical_features].head())"
      ],
      "metadata": {
        "id": "Yxxp-evMBnuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split Merged data(march and may) into training and validation datasets in 80% and 20% ratio split by timeline , not randomly\n",
        "\n",
        "# Split the merged data into training and validation sets based on the timeline (80/20 split)\n",
        "train_size = int(len(merged_data) * 0.8)\n",
        "train_data = merged_data[:train_size]\n",
        "val_data = merged_data[train_size:]\n",
        "\n",
        "print(\"Training data shape:\", train_data.shape)\n",
        "print(\"Validation data shape:\", val_data.shape)\n",
        "\n",
        "# Display the first few rows of the training and validation sets\n",
        "display(train_data)\n",
        "display(val_data)"
      ],
      "metadata": {
        "id": "O1NGxZTY1TTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separating Features and Target: Separating numerical features and target in each split (training and validation)\n",
        "\n",
        "# Separate features and target in training and validation sets\n",
        "\n",
        "X_train = train_data.drop('trip_duration', axis=1)\n",
        "y_train = train_data['trip_duration']\n",
        "X_val = val_data.drop('trip_duration', axis=1)\n",
        "y_val = val_data['trip_duration']\n",
        "\n",
        "#Further separate numerical features if needed.  Example:\n",
        "numerical_features_X_train = X_train[numerical_features]\n",
        "numerical_features_X_val = X_val[numerical_features]\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"X_val shape:\", X_val.shape)\n",
        "print(\"y_val shape:\", y_val.shape)\n",
        "\n",
        "print(\"\\nNumerical Features in X_train:\")\n",
        "display(numerical_features_X_train.head())\n",
        "print(\"\\nNumerical Features in X_val:\")\n",
        "display(numerical_features_X_val.head())"
      ],
      "metadata": {
        "id": "8dSDlRK4zgDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale numerical Features Independently for Training and Validation Sets:\n",
        "# Use a scaler to fit only on the training set, then transform both the training and validation features. Do not fit the scaler on the validation data to ensure that scaling is only informed by the training set.\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Initialize the scaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit the scaler on the training data numerical features only\n",
        "scaler.fit(numerical_features_X_train)\n",
        "\n",
        "# Transform both training and validation numerical features using the fitted scaler\n",
        "scaled_X_train = scaler.transform(numerical_features_X_train)\n",
        "scaled_X_val = scaler.transform(numerical_features_X_val)\n",
        "\n",
        "# Now you have scaled numerical features for both training and validation sets\n",
        "print(\"Scaled X_train shape:\", scaled_X_train.shape)\n",
        "print(\"Scaled X_val shape:\", scaled_X_val.shape)"
      ],
      "metadata": {
        "id": "6feT4vpvbMAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ISPLAY THE DATA AVOVE  READY TO TRAIN\n",
        "import pandas as pd\n",
        "# Display the scaled data (optional, but helpful for verification)\n",
        "print(\"\\nScaled Numerical Features in X_train:\")\n",
        "scaled_X_train_df = pd.DataFrame(scaled_X_train, columns=numerical_features)\n",
        "display(scaled_X_train_df.head())\n",
        "\n",
        "print(\"\\nScaled Numerical Features in X_val:\")\n",
        "scaled_X_val_df = pd.DataFrame(scaled_X_val, columns=numerical_features)\n",
        "display(scaled_X_val_df.head())"
      ],
      "metadata": {
        "id": "ECUS43BVnFJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for NaN in scaled_X_train and scaled_X_val dataompt:\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Check for NaN values in scaled_X_train\n",
        "nan_count_train = np.isnan(scaled_X_train).sum()\n",
        "print(f\"Number of NaN values in scaled_X_train: {nan_count_train}\")\n",
        "\n",
        "# Check for NaN values in scaled_X_val\n",
        "nan_count_val = np.isnan(scaled_X_val).sum()\n",
        "print(f\"Number of NaN values in scaled_X_val: {nan_count_val}\")\n"
      ],
      "metadata": {
        "id": "qAblZZBPwhrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#If NaN values exist, you might consider these options:\n",
        "# 1. Imputation: Replace NaN with mean, median, or another strategy\n",
        "if nan_count_train > 0:\n",
        "    from sklearn.impute import SimpleImputer\n",
        "    imputer = SimpleImputer(strategy='mean')  # Or 'median', etc.\n",
        "    scaled_X_train = imputer.fit_transform(scaled_X_train)\n",
        "\n",
        "    print(\"\\nScaled X_train after imputation:\")\n",
        "    scaled_X_train_df = pd.DataFrame(scaled_X_train, columns=numerical_features)\n",
        "    display(scaled_X_train_df.head())\n",
        "\n",
        "#The following if statement was incorrectly indented, causing the IndentationError.\n",
        "#Ensuring it is aligned with the previous if statement fixes the issue.\n",
        "if nan_count_val > 0:\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    scaled_X_val = imputer.fit_transform(scaled_X_val)\n",
        "    print(\"\\nScaled X_val after imputation:\")\n",
        "    scaled_X_val_df = pd.DataFrame(scaled_X_val, columns=numerical_features)\n",
        "    display(scaled_X_val_df.head())"
      ],
      "metadata": {
        "id": "_KGy8OlBxGxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for NaN in scaled_X_train and scaled_X_val data after imputation\n",
        "\n",
        "# Check for NaN values in scaled_X_train\n",
        "nan_count_train = np.isnan(scaled_X_train).sum()\n",
        "print(f\"Number of NaN values in scaled_X_train: {nan_count_train}\")\n",
        "\n",
        "# Check for NaN values in scaled_X_val\n",
        "nan_count_val = np.isnan(scaled_X_val).sum()\n",
        "print(f\"Number of NaN values in scaled_X_val: {nan_count_val}\")"
      ],
      "metadata": {
        "id": "-DVueiEexvbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the Models"
      ],
      "metadata": {
        "id": "xUMxhf64_UXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define and compile MLP (Multi-Layer Perceptron)models, optimizer is SGD, Mean Square Error (MSE) is loss function,\n",
        "#  the initial learning rate is 0.001， adding a Learning Rate Schedule , and give me the summary of the model then train the model. add early stopping, Regularization if overfitting happens\n",
        "#  present/plot the training_loss vs validation_loss of the model\n",
        "\n",
        "\n",
        "# Define the MLP model\n",
        "def create_mlp_model(input_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, activation='relu', input_dim=input_dim, kernel_regularizer=keras.regularizers.l2(0.001)))  # Added regularization\n",
        "    model.add(Dense(32, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))) # Added regularization\n",
        "    model.add(Dense(1))  # Output layer for regression\n",
        "    return model\n",
        "\n",
        "# Learning rate schedule\n",
        "def lr_scheduler(epoch, lr):\n",
        "    if epoch < 5:\n",
        "        return lr\n",
        "    else:\n",
        "        return lr * math.exp(-0.1)\n",
        "\n",
        "# Assuming scaled_X_train and scaled_X_val are defined from the previous code\n",
        "input_dim = scaled_X_train.shape[1]  # Get input dimension from scaled data\n",
        "model = create_mlp_model(input_dim)\n",
        "\n",
        "# Compile the model\n",
        "optimizer = SGD(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='mse')\n",
        "\n",
        "# Set up callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "lr_callback = LearningRateScheduler(lr_scheduler)\n",
        "\n",
        "\n",
        "# Model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(scaled_X_train, y_train, epochs=20, batch_size=32,\n",
        "                    validation_data=(scaled_X_val, y_val),\n",
        "                    callbacks=[early_stopping, lr_callback])\n",
        "\n",
        "\n",
        "#Plot training and validation loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DE4CwnheurNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pDefine and compile MLP (Multi-Layer Perceptron)models, optimizer is SGD, loss function is Mean Absolute Error (MAE),\n",
        "# the initial learning rate is 0.001， adding a Learning Rate Schedule , and give me the summary of the model then train the model. add early stopping, Regularization if overfitting happens\n",
        "# present/plot the training_loss vs validation_loss of the model above\n",
        "\n",
        "# Define the MLP model\n",
        "def create_mlp_model(input_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, activation='relu', input_dim=input_dim, kernel_regularizer=keras.regularizers.l2(0.001)))  # Added regularization\n",
        "    model.add(Dense(32, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))) # Added regularization\n",
        "    model.add(Dense(1))  # Output layer for regression\n",
        "    return model\n",
        "\n",
        "# Learning rate schedule\n",
        "def lr_scheduler(epoch, lr):\n",
        "    if epoch < 5:\n",
        "        return lr\n",
        "    else:\n",
        "        return lr * math.exp(-0.1)\n",
        "\n",
        "# Assuming scaled_X_train and scaled_X_val are defined from the previous code\n",
        "input_dim = scaled_X_train.shape[1]  # Get input dimension from scaled data\n",
        "model = create_mlp_model(input_dim)\n",
        "\n",
        "# Compile the model\n",
        "optimizer = SGD(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='mae')\n",
        "\n",
        "# Set up callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "lr_callback = LearningRateScheduler(lr_scheduler)\n",
        "\n",
        "\n",
        "# Model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(scaled_X_train, y_train, epochs=20, batch_size=32,\n",
        "                    validation_data=(scaled_X_val, y_val),\n",
        "                    callbacks=[early_stopping, lr_callback])\n",
        "\n",
        "\n",
        "#Plot training and validation loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ezAKpamuOpjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define the MLP model\n",
        "def create_mlp_model(input_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, activation='relu', input_dim=input_dim, kernel_regularizer=keras.regularizers.l2(0.001)))  # Added regularization\n",
        "    model.add(Dense(32, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))) # Added regularization\n",
        "    model.add(Dense(1))  # Output layer for regression\n",
        "    return model\n",
        "\n",
        "# Learning rate schedule\n",
        "def lr_scheduler(epoch, lr):\n",
        "    if epoch < 5:\n",
        "        return lr\n",
        "    else:\n",
        "        return lr * math.exp(-0.1)\n",
        "\n",
        "# Assuming scaled_X_train and scaled_X_val are defined from the previous code\n",
        "input_dim = scaled_X_train.shape[1]  # Get input dimension from scaled data\n",
        "model = create_mlp_model(input_dim)\n",
        "\n",
        "# Compile the model\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.001) # Use Adam optimizer\n",
        "model.compile(optimizer=optimizer, loss='mse')\n",
        "\n",
        "# Set up callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "lr_callback = LearningRateScheduler(lr_scheduler)\n",
        "\n",
        "\n",
        "# Model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(scaled_X_train, y_train, epochs=20, batch_size=32,\n",
        "                    validation_data=(scaled_X_val, y_val),\n",
        "                    callbacks=[early_stopping, lr_callback])\n",
        "\n",
        "\n",
        "#Plot training and validation loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4jurjPuAPa4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define the MLP model\n",
        "def create_mlp_model(input_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, activation='relu', input_dim=input_dim, kernel_regularizer=keras.regularizers.l2(0.001)))  # Added regularization\n",
        "    model.add(Dense(32, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))) # Added regularization\n",
        "    model.add(Dense(1))  # Output layer for regression\n",
        "    return model\n",
        "\n",
        "# Learning rate schedule\n",
        "def lr_scheduler(epoch, lr):\n",
        "    if epoch < 5:\n",
        "        return lr\n",
        "    else:\n",
        "        return lr * math.exp(-0.1)\n",
        "\n",
        "# Assuming scaled_X_train and scaled_X_val are defined from the previous code\n",
        "input_dim = scaled_X_train.shape[1]  # Get input dimension from scaled data\n",
        "model = create_mlp_model(input_dim)\n",
        "\n",
        "# Compile the model\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.001) # Use Adam optimizer\n",
        "model.compile(optimizer=optimizer, loss='mae')\n",
        "\n",
        "# Set up callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "lr_callback = LearningRateScheduler(lr_scheduler)\n",
        "\n",
        "\n",
        "# Model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(scaled_X_train, y_train, epochs=20, batch_size=32,\n",
        "                    validation_data=(scaled_X_val, y_val),\n",
        "                    callbacks=[early_stopping, lr_callback])\n",
        "\n",
        "\n",
        "#Plot training and validation loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "X_ruNfWGQfi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define and compile MLP (Multi-Layer Perceptron)models, optimizer is RMSProp, loss function is : Mean Square Error (MSE),\n",
        "#  the initial learning rate is 0.001， adding a Learning Rate Schedule , and give me the summary of the model then train the model. add early stopping, Regularization if overfitting happens\n",
        "# present/plot the training_loss vs validation_loss of the model above by utilizing TensorBoard GUI module\n",
        "\n",
        "# Define the MLP model\n",
        "def create_mlp_model(input_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, activation='relu', input_dim=input_dim, kernel_regularizer=keras.regularizers.l2(0.001)))  # Added regularization\n",
        "    model.add(Dense(32, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))) # Added regularization\n",
        "    model.add(Dense(1))  # Output layer for regression\n",
        "    return model\n",
        "\n",
        "# Learning rate schedule\n",
        "def lr_scheduler(epoch, lr):\n",
        "    if epoch < 5:\n",
        "        return lr\n",
        "    else:\n",
        "        return lr * math.exp(-0.1)\n",
        "\n",
        "# Assuming scaled_X_train and scaled_X_val are defined from the previous code\n",
        "input_dim = scaled_X_train.shape[1]  # Get input dimension from scaled data\n",
        "model = create_mlp_model(input_dim)\n",
        "\n",
        "# Compile the model with RMSprop optimizer and MSE loss\n",
        "optimizer = keras.optimizers.RMSprop(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='mse')\n",
        "\n",
        "# Set up callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "lr_callback = LearningRateScheduler(lr_scheduler)\n",
        "\n",
        "# Model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(scaled_X_train, y_train, epochs=20, batch_size=32,\n",
        "                    validation_data=(scaled_X_val, y_val),\n",
        "                    callbacks=[early_stopping, lr_callback])\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dFKcHhqzQi7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Define and compile MLP (Multi-Layer Perceptron)models, optimizer is RMSProp, loss function is : Mean Square Error (MSE),\n",
        "#  the initial learning rate is 0.001， adding a Learning Rate Schedule , and give me the summary of the model then train the model. add early stopping, Regularization if overfitting happens\n",
        "# present/plot the training_loss vs validation_loss of the model above by utilizing TensorBoard GUI module\n",
        "\n",
        "# Define the MLP model\n",
        "def create_mlp_model(input_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, activation='relu', input_dim=input_dim, kernel_regularizer=keras.regularizers.l2(0.001)))  # Added regularization\n",
        "    model.add(Dense(32, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))) # Added regularization\n",
        "    model.add(Dense(1))  # Output layer for regression\n",
        "    return model\n",
        "\n",
        "# Learning rate schedule\n",
        "def lr_scheduler(epoch, lr):\n",
        "    if epoch < 5:\n",
        "        return lr\n",
        "    else:\n",
        "        return lr * math.exp(-0.1)\n",
        "\n",
        "# Assuming scaled_X_train and scaled_X_val are defined from the previous code\n",
        "input_dim = scaled_X_train.shape[1]  # Get input dimension from scaled data\n",
        "model = create_mlp_model(input_dim)\n",
        "\n",
        "# Compile the model with RMSprop optimizer and MSE loss\n",
        "optimizer = keras.optimizers.RMSprop(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='mae')\n",
        "\n",
        "# Set up callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "lr_callback = LearningRateScheduler(lr_scheduler)\n",
        "\n",
        "# Model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(scaled_X_train, y_train, epochs=20, batch_size=32,\n",
        "                    validation_data=(scaled_X_val, y_val),\n",
        "                    callbacks=[early_stopping,lr_callback])\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lkbKNgMbQ99V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression"
      ],
      "metadata": {
        "id": "C6FE_PPNNmjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define and compile Linear Regression (TF/Keras Sequential model w/ no hidden layers) models, optimizer is  SGD, loss function is Mean Square Error (MSE),\n",
        "#  the initial learning rate is 0.001 , and give me the summary of the model then train the model. add early stopping, and Regularization if overfitting happens\n",
        "# present/plot the training_loss vs validation_loss of the model above by utilizing TensorBoard GUI module\n",
        "\n",
        "# Define the linear regression model\n",
        "def create_linear_regression_model(input_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(1, input_dim=input_dim))  # No hidden layers, directly to output\n",
        "    return model\n",
        "\n",
        "# Assuming scaled_X_train and scaled_X_val are defined from the previous code\n",
        "input_dim = scaled_X_train.shape[1]\n",
        "model = create_linear_regression_model(input_dim)\n",
        "\n",
        "# Compile the model\n",
        "optimizer = SGD(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='mse')\n",
        "\n",
        "# Set up callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(scaled_X_train, y_train, epochs=20, batch_size=32,\n",
        "                    validation_data=(scaled_X_val, y_val),\n",
        "                    callbacks=[early_stopping])\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3gepfAWBQ_V6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define and compile Linear Regression (TF/Keras Sequential model w/ no hidden layers) models, optimizer is  SGD, loss function is Mean Square Error (MAE),\n",
        "#  the initial learning rate is 0.001 , and give me the summary of the model then train the model. add early stopping, and Regularization if overfitting happens\n",
        "# present/plot the training_loss vs validation_loss of the model above by utilizing TensorBoard GUI module\n",
        "\n",
        "# Define the linear regression model\n",
        "def create_linear_regression_model(input_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(1, input_dim=input_dim))  # No hidden layers, directly to output\n",
        "    return model\n",
        "\n",
        "# Assuming scaled_X_train and scaled_X_val are defined from the previous code\n",
        "input_dim = scaled_X_train.shape[1]\n",
        "model = create_linear_regression_model(input_dim)\n",
        "\n",
        "# Compile the model\n",
        "optimizer = SGD(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='mae')\n",
        "\n",
        "# Set up callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(scaled_X_train, y_train, epochs=20, batch_size=32,\n",
        "                    validation_data=(scaled_X_val, y_val),\n",
        "                    callbacks=[early_stopping])\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OCmM4wOASVnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Define and compileLinear Regression (TF/Keras Sequential model w/ no hidden layers) models, optimizer is  Adam, loss function is Mean Square Error (MSE),\n",
        "#  the initial learning rate is 0.001 , and give me the summary of the model then train the model. add early stopping, and Regularization if overfitting happens\n",
        "# present/plot the training_loss vs validation_loss of the model above by utilizing TensorBoard GUI module\n",
        "\n",
        "# Define the linear regression model\n",
        "def create_linear_regression_model(input_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(1, input_dim=input_dim))  # No hidden layers, directly to output\n",
        "    return model\n",
        "\n",
        "# Assuming scaled_X_train and scaled_X_val are defined from the previous code\n",
        "input_dim = scaled_X_train.shape[1]\n",
        "model = create_linear_regression_model(input_dim)\n",
        "\n",
        "# Compile the model with Adam optimizer and MSE loss\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='mse')\n",
        "\n",
        "# Set up callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(scaled_X_train, y_train, epochs=20, batch_size=32,\n",
        "                    validation_data=(scaled_X_val, y_val),\n",
        "                    callbacks=[early_stopping])\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MleqWDtJSeWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Define and compileLinear Regression (TF/Keras Sequential model w/ no hidden layers) models, optimizer is  SGD, loss function is MAE,\n",
        "#  the initial learning rate is 0.001 , and give me the summary of the model then train the model. add early stopping, and Regularization if overfitting happens\n",
        "# present/plot the training_loss vs validation_loss of the model above by utilizing TensorBoard GUI module\n",
        "\n",
        "# Define the linear regression model\n",
        "def create_linear_regression_model(input_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(1, input_dim=input_dim))  # No hidden layers, directly to output\n",
        "    return model\n",
        "\n",
        "# Assuming scaled_X_train and scaled_X_val are defined from the previous code\n",
        "input_dim = scaled_X_train.shape[1]\n",
        "model = create_linear_regression_model(input_dim)\n",
        "\n",
        "# Compile the model with Adam optimizer and MAE loss\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='mae')\n",
        "\n",
        "# Set up callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(scaled_X_train, y_train, epochs=20, batch_size=32,\n",
        "                    validation_data=(scaled_X_val, y_val),\n",
        "                    callbacks=[early_stopping])\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XkrrNqpZSp9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define and compileLinear Regression (TF/Keras Sequential model w/ no hidden layers) models, optimizer is  RMSProp, loss function is Mean Square Error (MSE),\n",
        "#  the initial learning rate is 0.001 , and give me the summary of the model then train the model. add early stopping, and Regularization if overfitting happens\n",
        "# present/plot the training_loss vs validation_loss of the model above by utilizing TensorBoard GUI module\n",
        "\n",
        "# Define the linear regression model\n",
        "def create_linear_regression_model(input_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(1, input_dim=input_dim))  # No hidden layers, directly to output\n",
        "    return model\n",
        "\n",
        "# Assuming scaled_X_train and scaled_X_val are defined from the previous code\n",
        "input_dim = scaled_X_train.shape[1]\n",
        "model = create_linear_regression_model(input_dim)\n",
        "\n",
        "# Compile the model with RMSprop optimizer and MSE loss\n",
        "optimizer = keras.optimizers.RMSprop(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='mse')\n",
        "\n",
        "# Set up callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(scaled_X_train, y_train, epochs=20, batch_size=32,\n",
        "                    validation_data=(scaled_X_val, y_val),\n",
        "                    callbacks=[early_stopping)\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ASZrxLBqS1_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define and compileLinear Regression (TF/Keras Sequential model w/ no hidden layers) models, optimizer is  RMSProp, loss function is Mean Square Error (MSE),\n",
        "#  the initial learning rate is 0.001 , and give me the summary of the model then train the model. add early stopping, and Regularization if overfitting happens\n",
        "# present/plot the training_loss vs validation_loss of the model above by utilizing TensorBoard GUI module\n",
        "\n",
        "# Define the linear regression model\n",
        "def create_linear_regression_model(input_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(1, input_dim=input_dim))  # No hidden layers, directly to output\n",
        "    return model\n",
        "\n",
        "# Assuming scaled_X_train and scaled_X_val are defined from the previous code\n",
        "input_dim = scaled_X_train.shape[1]\n",
        "model = create_linear_regression_model(input_dim)\n",
        "\n",
        "# Compile the model with RMSprop optimizer and MSE loss\n",
        "optimizer = keras.optimizers.RMSprop(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='mae')\n",
        "\n",
        "# Set up callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "\n",
        "# Model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(scaled_X_train, y_train, epochs=20, batch_size=32,\n",
        "                    validation_data=(scaled_X_val, y_val),\n",
        "                    callbacks=[early_stopping])\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bgNl_eF5TI5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DNN"
      ],
      "metadata": {
        "id": "jh9mFDuNNYxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Define and compile DNN (Deep Neural Network with at least 2 hidden layers) w/ no hidden layers) models, optimizer is  SGD, loss function is Mean Square Error (MSE),\n",
        "#  the initial learning rate is 0.0001, using ReduceLROnPlateau to adjust learning rate , and give me the summary of the model then train the model. add early stopping, and Dropout if overfitting happens\n",
        "# present/plot the training_loss vs validation_loss of the model above by utilizing TensorBoard GUI module\n",
        "\n",
        "%load_ext tensorboard\n",
        "import datetime\n",
        "\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "# Assuming scaled_X_train, y_train, scaled_X_val, y_val are defined\n",
        "# and input_dim is calculated as scaled_X_train.shape[1]\n",
        "\n",
        "def create_dnn_model(input_dim):\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='relu', input_shape=(input_dim,)),\n",
        "        tf.keras.layers.Dropout(0.2),  # Add dropout for regularization\n",
        "        Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.2), # Add dropout for regularization\n",
        "        Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "input_dim = scaled_X_train.shape[1]\n",
        "model = create_dnn_model(input_dim)\n",
        "\n",
        "optimizer = SGD(learning_rate=0.0001)\n",
        "model.compile(optimizer=optimizer, loss='mse') # or 'mae'\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(scaled_X_train, y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(scaled_X_val, y_val),\n",
        "                    callbacks=[early_stopping,reduce_lr])\n",
        "\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "%tensorboard --logdir logs/fit"
      ],
      "metadata": {
        "id": "L6TWuRHQTKOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define and compile DNN (Deep Neural Network with at least 2 hidden layers) w/ no hidden layers) models, optimizer is  SGD, loss function is Mean Square Error (MSE),\n",
        "# the initial learning rate is 0.0001, using ReduceLROnPlateau to adjust learning rate , and give me the summary of the model then train the model. add early stopping, and Dropout if overfitting happens\n",
        "# present/plot the training_loss vs validation_loss of the model above by utilizing TensorBoard GUI module\n",
        "# and compare the prediction value and actual value\n",
        "\n",
        "# Assuming scaled_X_train, y_train, scaled_X_val, y_val, and input_dim are defined\n",
        "from tensorflow.keras.layers import Dropout # Import Dropout from keras.layers\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau # Import ReduceLROnPlateau\n",
        "def create_dnn_model(input_dim):\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='relu', input_shape=(input_dim,)),\n",
        "        Dropout(0.2),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "input_dim = scaled_X_train.shape[1]\n",
        "model = create_dnn_model(input_dim)\n",
        "\n",
        "optimizer = SGD(learning_rate=0.0001)\n",
        "model.compile(optimizer=optimizer, loss='mae')\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(scaled_X_train, y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(scaled_X_val, y_val),\n",
        "                    callbacks=[early_stopping,reduce_lr])\n",
        "\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs/fit\n"
      ],
      "metadata": {
        "id": "4Ub8UqJZjlLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Define and compile DNN (Deep Neural Network with at least 2 hidden layers) w/ no hidden layers) models, optimizer is  Adam, loss function is Mean Square Error (MSE),\n",
        "#  the initial learning rate is 0.0001, using ReduceLROnPlateau to adjust learning rate , and give me the summary of the model then train the model. add early stopping, and Dropout if overfitting happens\n",
        "# present/plot the training_loss vs validation_loss of the model above by utilizing TensorBoard GUI module\n",
        "\n",
        "# Assuming scaled_X_train, y_train, scaled_X_val, y_val are defined\n",
        "# and input_dim is calculated as scaled_X_train.shape[1]\n",
        "\n",
        "def create_dnn_model(input_dim):\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='relu', input_shape=(input_dim,)),\n",
        "        tf.keras.layers.Dropout(0.2),  # Add dropout for regularization\n",
        "        Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.2), # Add dropout for regularization\n",
        "        Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "input_dim = scaled_X_train.shape[1]\n",
        "model = create_dnn_model(input_dim)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001) # Use Adam optimizer\n",
        "model.compile(optimizer=optimizer, loss='mse') # Use MSE loss function\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(scaled_X_train, y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(scaled_X_val, y_val),\n",
        "                    callbacks=[early_stopping, tensorboard_callback, reduce_lr])\n",
        "\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "%tensorboard --logdir logs/fit"
      ],
      "metadata": {
        "id": "HYT3gpeGVwk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define and compile DNN (Deep Neural Network with at least 2 hidden layers) w/ no hidden layers) models, optimizer is  Adam, loss function is Mean Square Error (MSE),\n",
        "#  the initial learning rate is 0.0001, using ReduceLROnPlateau to adjust learning rate , and give me the summary of the model then train the model. add early stopping, and Dropout if overfitting happens\n",
        "# present/plot the training_loss vs validation_loss of the model above by utilizing TensorBoard GUI module\n",
        "\n",
        "# Assuming scaled_X_train, y_train, scaled_X_val, y_val are defined\n",
        "# and input_dim is calculated as scaled_X_train.shape[1]\n",
        "\n",
        "# Define the log directory for TensorBoard\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "\n",
        "def create_dnn_model(input_dim):\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='relu', input_shape=(input_dim,)),\n",
        "        tf.keras.layers.Dropout(0.2),  # Add dropout for regularization\n",
        "        Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.2), # Add dropout for regularization\n",
        "        Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "input_dim = scaled_X_train.shape[1]\n",
        "model = create_dnn_model(input_dim)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001) # Use Adam optimizer\n",
        "model.compile(optimizer=optimizer, loss='mae') # Use MSE loss function\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(scaled_X_train, y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(scaled_X_val, y_val),\n",
        "                    callbacks=[early_stopping, tensorboard_callback, reduce_lr])\n",
        "\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "%tensorboard --logdir logs/fit"
      ],
      "metadata": {
        "id": "yip5lf9YV_G2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Define and compile DNN (Deep Neural Network with at least 2 hidden layers) w/ no hidden layers) models, optimizer is  RMSProp, loss function is Mean Square Error (MSE),\n",
        "#  the initial learning rate is 0.0001, using ReduceLROnPlateau to adjust learning rate , and give me the summary of the model then train the model. add early stopping, and Dropout if overfitting happens\n",
        "# present/plot the training_loss vs validation_loss of the model above by utilizing TensorBoard GUI module\n",
        "\n",
        "%load_ext tensorboard\n",
        "\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "# Assuming scaled_X_train, y_train, scaled_X_val, y_val are defined\n",
        "# and input_dim is calculated as scaled_X_train.shape[1]\n",
        "\n",
        "def create_dnn_model(input_dim):\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='relu', input_shape=(input_dim,)),\n",
        "        tf.keras.layers.Dropout(0.2),  # Add dropout for regularization\n",
        "        Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.2), # Add dropout for regularization\n",
        "        Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "input_dim = scaled_X_train.shape[1]\n",
        "model = create_dnn_model(input_dim)\n",
        "\n",
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.0001)\n",
        "model.compile(optimizer=optimizer, loss='mse') # or 'mae'\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(scaled_X_train, y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(scaled_X_val, y_val),\n",
        "                    callbacks=[early_stopping, tensorboard_callback, reduce_lr])\n",
        "\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "%tensorboard --logdir logs/fit"
      ],
      "metadata": {
        "id": "h19UzwWiWE1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Define and compile DNN (Deep Neural Network with at least 2 hidden layers) w/ no hidden layers) models, optimizer is  RMSProp, loss function is Mean Square Error (MaE),\n",
        "#  the initial learning rate is 0.0001, using ReduceLROnPlateau to adjust learning rate , and give me the summary of the model then train the model. add early stopping, and Dropout if overfitting happens\n",
        "# present/plot the training_loss vs validation_loss of the model above by utilizing TensorBoard GUI module\n",
        "\n",
        "%load_ext tensorboard\n",
        "\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "# Assuming scaled_X_train, y_train, scaled_X_val, y_val are defined\n",
        "# and input_dim is calculated as scaled_X_train.shape[1]\n",
        "\n",
        "def create_dnn_model(input_dim):\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='relu', input_shape=(input_dim,)),\n",
        "        tf.keras.layers.Dropout(0.2),  # Add dropout for regularization\n",
        "        Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.2), # Add dropout for regularization\n",
        "        Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "input_dim = scaled_X_train.shape[1]\n",
        "model = create_dnn_model(input_dim)\n",
        "\n",
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.0001)\n",
        "model.compile(optimizer=optimizer, loss='mae') # or 'mae'\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(scaled_X_train, y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(scaled_X_val, y_val),\n",
        "                    callbacks=[early_stopping, tensorboard_callback, reduce_lr])\n",
        "\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "%tensorboard --logdir logs/fit"
      ],
      "metadata": {
        "id": "c-2uhyCzWRAq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}